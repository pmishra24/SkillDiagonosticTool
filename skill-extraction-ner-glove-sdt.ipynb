{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11383001,"sourceType":"datasetVersion","datasetId":7127504},{"sourceId":11383018,"sourceType":"datasetVersion","datasetId":7127515}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install skillNer\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('punkt_tab')\n\nGLOVE_PATH = \"/kaggle/input/skill-extractor/glove.6B.100d.txt\"  # Path to GloVe file\nSKILLS_EXCEL = \"/kaggle/input/skill-extractor/Merged_Unique_Skills.csv\"      # Excel with 'Skill' column\nINPUT_CSV = \"/kaggle/input/skill-extractor/Linkedin_data_no_duplicates v1.0.csv\"      # Input CSV file with a column \"job_desc\"\nOUTPUT_EXCEL = \"ngram_skills.xlsx\"\nOUTPUT_CSV = \"job_posting_with_skills.csv\"\nOUTPUT_GLOVE_EXCEL = \"matched_skills_glove.xlsx\"\nOUTPUT_NER_EXCEL = \"matched_skills_ner.xlsx\"\nEMBEDDING_DIM = 100\nSIMILARITY_THRESHOLD = 0.89\n\ndef load_glove_embeddings(file_path):\n    embeddings = {}\n    with open(file_path, 'r', encoding='utf8') as f:\n        for line in f:\n            parts = line.split()\n            word = parts[0]\n            vector = list(map(float, parts[1:]))\n            embeddings[word] = vector\n    return embeddings\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:43:04.920512Z","iopub.execute_input":"2025-04-13T10:43:04.920671Z","iopub.status.idle":"2025-04-13T10:43:15.621378Z","shell.execute_reply.started":"2025-04-13T10:43:04.920656Z","shell.execute_reply":"2025-04-13T10:43:15.620615Z"}},"outputs":[{"name":"stdout","text":"Collecting skillNer\n  Downloading skillNer-1.0.3.tar.gz (24 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from skillNer) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from skillNer) (2.2.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from skillNer) (3.9.1)\nRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (from skillNer) (3.7.5)\nRequirement already satisfied: jellyfish in /usr/local/lib/python3.11/dist-packages (from skillNer) (1.1.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->skillNer) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->skillNer) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->skillNer) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->skillNer) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->skillNer) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->skillNer) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->skillNer) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->skillNer) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->skillNer) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->skillNer) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->skillNer) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->skillNer) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->skillNer) (2025.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (1.0.12)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (0.15.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (2.11.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (75.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy->skillNer) (3.5.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->skillNer) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->skillNer) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->skillNer) (2.33.1)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->skillNer) (4.13.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->skillNer) (0.4.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->skillNer) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->skillNer) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->skillNer) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->skillNer) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy->skillNer) (2025.1.31)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->skillNer) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->skillNer) (0.1.5)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->skillNer) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy->skillNer) (14.0.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->skillNer) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->skillNer) (7.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy->skillNer) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->skillNer) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->skillNer) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->skillNer) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->skillNer) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->skillNer) (2024.2.0)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->skillNer) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->skillNer) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->skillNer) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->skillNer) (1.17.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->skillNer) (0.1.2)\nBuilding wheels for collected packages: skillNer\n  Building wheel for skillNer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for skillNer: filename=skillNer-1.0.3-py3-none-any.whl size=25625 sha256=4e94c84de05134b24976f92990d5b678950f7e98c639d14ece83a2cdc3a2534f\n  Stored in directory: /root/.cache/pip/wheels/62/01/98/b823d6086aacca94c7d9083081aee3effca467bedb621410e9\nSuccessfully built skillNer\nInstalling collected packages: skillNer\nSuccessfully installed skillNer-1.0.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"glove = load_glove_embeddings(GLOVE_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:43:22.558783Z","iopub.execute_input":"2025-04-13T10:43:22.559053Z","iopub.status.idle":"2025-04-13T10:43:33.580490Z","shell.execute_reply.started":"2025-04-13T10:43:22.559032Z","shell.execute_reply":"2025-04-13T10:43:33.579647Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom numpy.linalg import norm\n\n# For NER-based extraction with GPU support\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nfrom skillNer.cleaner import Cleaner\nimport spacy\nimport torch\n\n# For progress bar\nfrom tqdm import tqdm\n\n# Uncomment these if needed:\n# nltk.download('punkt')\n# nltk.download('stopwords')\n\n# ---------------------------------------------\n#        GloVe-Based Extraction Functions\n# ---------------------------------------------\nps = PorterStemmer()\n\ndef get_stem_set(text):\n    \"\"\"\n    Returns a set of stemmed tokens for a given text.\n    Uses strict normalization (removes punctuation but keeps stopwords) and stems each token.\n    \"\"\"\n    text = re.sub(r'[-]', ' ', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    tokens = word_tokenize(text.lower().strip())\n    return set(ps.stem(token) for token in tokens)\n\ndef load_glove_embeddings(file_path):\n    \"\"\"Loads GloVe embeddings from a file.\"\"\"\n    print(\"Loading GloVe embeddings...\")\n    embeddings = {}\n    with open(file_path, 'r', encoding='utf8') as f:\n        for line in f:\n            parts = line.strip().split()\n            word = parts[0]\n            vector = list(map(float, parts[1:]))\n            embeddings[word] = vector\n    print(f\"Loaded {len(embeddings)} word vectors.\")\n    return embeddings\n\nstop_words = set(stopwords.words('english'))\n\ndef preprocess(text):\n    \"\"\"\n    Remove hyphens (convert to space), then remove non-alphabetic characters,\n    lowercase, tokenize, and remove stopwords.\n    Returns a list of tokens.\n    \"\"\"\n    text = re.sub(r'[-]', ' ', text)\n    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n    tokens = word_tokenize(text.lower())\n    return [t for t in tokens if t not in stop_words]\n\ndef normalize_text(text):\n    \"\"\"\n    Normalizes text for matching by preprocessing and then joining tokens.\n    (Stopwords are removed.)\n    \"\"\"\n    tokens = preprocess(text)\n    return ' '.join(tokens)\n\ndef strict_normalize(text):\n    \"\"\"\n    Strict normalization: replace hyphens with space, remove punctuation,\n    lowercase the text, but do not remove stopwords.\n    \"\"\"\n    text = re.sub(r'[-]', ' ', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text.lower().strip()\n\ndef average_vector(tokens, glove, dim=100):\n    \"\"\"\n    Computes an average vector for a list of tokens using the provided GloVe embeddings.\n    \"\"\"\n    vectors = [glove[t] for t in tokens if t in glove]\n    return np.mean(vectors, axis=0) if vectors else np.zeros(dim)\n\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Returns cosine similarity between two vectors.\n    \"\"\"\n    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2) + 1e-8)\n\ndef generate_ngrams(tokens, n_range=(1, 3)):\n    \"\"\"\n    Generates 1–3 word n-grams from a list of tokens.\n    \"\"\"\n    ngrams = []\n    for n in range(n_range[0], n_range[1] + 1):\n        ngrams += [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n    return ngrams\n\ndef match_phrases_to_skills(job_desc, skill_list, glove, threshold=0.6, dim=100):\n    \"\"\"\n    Matches phrases in the job description to a list of skills using cosine similarity of GloVe embeddings.\n    Returns a sorted list of tuples: (Skill, Similarity, Matched Phrase)\n    \"\"\"\n    tokens = preprocess(job_desc)\n    phrases = generate_ngrams(tokens, n_range=(1, 3))\n    \n    normalized_skill_dict = {}\n    for skill in skill_list:\n        norm_skill = normalize_text(skill)\n        if 1 <= len(norm_skill.split()) <= 3:\n            normalized_skill_dict[norm_skill] = skill\n\n    matched_phrases = []\n    \n    for phrase in phrases:\n        norm_phrase = normalize_text(phrase)\n        if norm_phrase in normalized_skill_dict:\n            candidate = normalized_skill_dict[norm_phrase]\n            extracted_stems = get_stem_set(phrase)\n            candidate_stems = get_stem_set(candidate)\n            if len(extracted_stems) < len(candidate_stems) and extracted_stems.issubset(candidate_stems):\n                continue\n            matched_phrases.append((candidate, 1.0, phrase))\n        else:\n            best_match = None\n            best_score = 0\n            for norm_skill, orig_skill in normalized_skill_dict.items():\n                skill_tokens = preprocess(orig_skill)\n                skill_vec = average_vector(skill_tokens, glove, dim)\n                phrase_tokens = phrase.split()\n                phrase_vec = average_vector(phrase_tokens, glove, dim)\n                sim = cosine_similarity(skill_vec, phrase_vec)\n                if sim > best_score:\n                    best_score = sim\n                    best_match = orig_skill\n            if best_score >= threshold:\n                extracted_stems = get_stem_set(phrase)\n                candidate_stems = get_stem_set(best_match)\n                if len(extracted_stems) < len(candidate_stems) and extracted_stems.issubset(candidate_stems):\n                    continue\n                matched_phrases.append((best_match, best_score, phrase))\n    \n    return sorted(matched_phrases, key=lambda x: -x[1])\n\ndef extract_glove_skills(job_desc, glove, skill_list, similarity_threshold=0.6, emb_dim=100):\n    \"\"\"\n    Extracts skills from a job description using the GloVe-based matching method.\n    Returns a semicolon-separated string of \"skill:score\" pairs.\n    \"\"\"\n    glove_results = match_phrases_to_skills(job_desc, skill_list, glove, threshold=similarity_threshold, dim=emb_dim)\n    unique_matches = {}\n    for skill, score, _ in glove_results:\n        if skill not in unique_matches or score > unique_matches[skill]:\n            unique_matches[skill] = score\n    return \"; \".join([f\"{skill}:{unique_matches[skill]:.2f}\" for skill in unique_matches])\n\n# ---------------------------------------------\n#          NER-Based Extraction Functions\n# ---------------------------------------------\ndef extract_ner_skills(job_desc, ner_pipeline, cleaner):\n    \"\"\"\n    Extracts skills from a job description using a Hugging Face NER pipeline.\n    Returns a semicolon-separated string of \"skill:score\" pairs.\n    \"\"\"\n    job_desc_clean = cleaner(job_desc)\n    ner_results = ner_pipeline(job_desc_clean)\n    unique_skills_ner = {}\n    for ent in ner_results:\n        skill = ent['word']\n        score = ent['score']\n        if skill in unique_skills_ner:\n            unique_skills_ner[skill] = max(unique_skills_ner[skill], score)\n        else:\n            unique_skills_ner[skill] = score\n    return \"; \".join([f\"{skill}:{unique_skills_ner[skill]:.2f}\" for skill in unique_skills_ner])\n\n# ---------------------------------------------\n#            Main Processing Script\n# ---------------------------------------------\nif __name__ == \"__main__\":\n    print(\"========== Starting Processing ==========\")\n    # ---------------------------\n    # 1. Set Paths and Parameters\n    # ---------------------------                     # GloVe embedding dimensions\n\n    # ---------------------------\n    # 2. Load GloVe Embeddings & Skill List\n    # ---------------------------    \n    print(\"Loading skill list...\")\n    df_skills = pd.read_csv(SKILLS_EXCEL)\n    full_skill_list = df_skills['Skill'].dropna().tolist()\n    filtered_skill_list = [skill for skill in full_skill_list if 1 <= len(normalize_text(skill).split()) <= 3]\n    print(f\"Filtered skill list: {len(filtered_skill_list)} skills retained.\")\n\n    # ---------------------------\n    # 3. Set Up NER Pipeline with GPU Optimization\n    # ---------------------------\n    device = 0 if torch.cuda.is_available() else -1\n    print(f\"Setting up NER pipeline (using {'GPU' if device==0 else 'CPU'})...\")\n    nlp = spacy.load(\"en_core_web_sm\")\n    cleaner = Cleaner(\n        to_lowercase=True,\n        include_cleaning_functions=[\n            \"remove_punctuation\", \"remove_extra_space\", \"remove_redundant\", \"lem_text(nlp)\"\n        ]\n    )\n    model_name = \"algiraldohe/lm-ner-linkedin-skills-recognition\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForTokenClassification.from_pretrained(model_name)\n    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device, aggregation_strategy=\"simple\")\n    \n    # ---------------------------\n    # 4. Read Input CSV and Process Each Job Description\n    # ---------------------------\n    print(\"Reading input CSV file...\")\n    headers = pd.read_csv(INPUT_CSV, nrows=0).columns\n\n    # Then read from line 250 onwards, keeping the headers manually\n    df_jobs = pd.read_csv(INPUT_CSV, skiprows=range(1, 249), header=None, names=headers)\n\n    # df_jobs = pd.read_csv(INPUT_CSV, skiprows=250)\n    \n    if \"description\" not in df_jobs.columns:\n        raise ValueError(\"Input CSV must contain a column named 'description'\")\n    \n    # Initialize new columns\n    df_jobs[\"Glove_Skills\"] = \"\"\n    df_jobs[\"NER_Skills\"] = \"\"\n    \n    print(\"Extracting skills for each job description (this may take a while)...\")\n    # Process rows one at a time and save progress every 50 jobs.\n    for idx in tqdm(df_jobs.index, desc=\"Processing jobs\"):\n        job_description = df_jobs.at[idx, \"description\"]\n        glove_skills = extract_glove_skills(job_description, glove, filtered_skill_list, SIMILARITY_THRESHOLD, EMBEDDING_DIM)\n        ner_skills = extract_ner_skills(job_description, ner_pipeline, cleaner)\n        df_jobs.at[idx, \"Glove_Skills\"] = glove_skills\n        df_jobs.at[idx, \"NER_Skills\"] = ner_skills\n        \n        # Save progress every 50 jobs\n        if (idx + 1) % 50 == 0:\n            df_jobs.to_excel(OUTPUT_EXCEL, index=False)\n            print(f\"Saved progress after processing {idx + 1} job descriptions.\")\n    \n    # Final save after processing all job descriptions.\n    df_jobs.to_excel(OUTPUT_EXCEL, index=False)\n    print(f\"Processing completed. Enriched Excel saved as: {OUTPUT_EXCEL}\")\n    print(\"========== All Done ==========\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:52:09.386607Z","iopub.execute_input":"2025-04-13T10:52:09.386905Z","execution_failed":"2025-04-13T17:49:27.630Z"}},"outputs":[{"name":"stdout","text":"========== Starting Processing ==========\nLoading skill list...\nFiltered skill list: 1233 skills retained.\nSetting up NER pipeline (using GPU)...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Reading input CSV file...\nExtracting skills for each job description (this may take a while)...\n","output_type":"stream"},{"name":"stderr","text":"Processing jobs:   3%|▎         | 10/356 [29:22<13:53:54, 144.61s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\nProcessing jobs:  14%|█▍        | 50/356 [2:38:29<20:28:15, 240.84s/it]","output_type":"stream"},{"name":"stdout","text":"Saved progress after processing 50 job descriptions.\n","output_type":"stream"},{"name":"stderr","text":"Processing jobs:  28%|██▊       | 100/356 [5:35:57<10:10:26, 143.07s/it]","output_type":"stream"},{"name":"stdout","text":"Saved progress after processing 100 job descriptions.\n","output_type":"stream"},{"name":"stderr","text":"Processing jobs:  36%|███▌      | 127/356 [6:54:56<11:07:40, 174.94s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"ssasa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T10:51:18.124311Z","iopub.execute_input":"2025-04-13T10:51:18.124629Z","iopub.status.idle":"2025-04-13T10:51:18.188147Z","shell.execute_reply.started":"2025-04-13T10:51:18.124604Z","shell.execute_reply":"2025-04-13T10:51:18.187608Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                              title  \\\n0  Unnamed: 0                                              title   \n1        1085  Assistant Director - Energy Innovation (Denver...   \n2        1086                                      Field Landman   \n3        1087                     Energy Consultant - Dallas, TX   \n4        1088             Manager, Clean Energy Asset Management   \n\n                 company       location  \\\n0                company       location   \n1      State of Colorado     Denver, CO   \n2  Scout Energy Partners    Ulysses, KS   \n3                Suntria     Dallas, TX   \n4                   Meta  United States   \n\n                                            job_link  \\\n0                                           job_link   \n1  https://www.linkedin.com/jobs/view/assistant-d...   \n2  https://www.linkedin.com/jobs/view/field-landm...   \n3  https://www.linkedin.com/jobs/view/energy-cons...   \n4  https://www.linkedin.com/jobs/view/manager-cle...   \n\n                                         description  \\\n0                                        description   \n1  Department Information\\nOPEN ONLY TO CURRENT R...   \n2  Duties & Responsibilities\\n Landowner Relation...   \n3  Suntria is searching for a passionate and know...   \n4  The Manager of Clean Energy Asset Management w...   \n\n                                              skills  \n0                                             skills  \n1  renewable energy, geothermal, sustainability, ...  \n2                                                 ai  \n3  renewable energy, energy efficiency, sustainab...  \n4  renewable energy, solar, wind, sustainability,...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>company</th>\n      <th>location</th>\n      <th>job_link</th>\n      <th>description</th>\n      <th>skills</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Unnamed: 0</td>\n      <td>title</td>\n      <td>company</td>\n      <td>location</td>\n      <td>job_link</td>\n      <td>description</td>\n      <td>skills</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1085</td>\n      <td>Assistant Director - Energy Innovation (Denver...</td>\n      <td>State of Colorado</td>\n      <td>Denver, CO</td>\n      <td>https://www.linkedin.com/jobs/view/assistant-d...</td>\n      <td>Department Information\\nOPEN ONLY TO CURRENT R...</td>\n      <td>renewable energy, geothermal, sustainability, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1086</td>\n      <td>Field Landman</td>\n      <td>Scout Energy Partners</td>\n      <td>Ulysses, KS</td>\n      <td>https://www.linkedin.com/jobs/view/field-landm...</td>\n      <td>Duties &amp; Responsibilities\\n Landowner Relation...</td>\n      <td>ai</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1087</td>\n      <td>Energy Consultant - Dallas, TX</td>\n      <td>Suntria</td>\n      <td>Dallas, TX</td>\n      <td>https://www.linkedin.com/jobs/view/energy-cons...</td>\n      <td>Suntria is searching for a passionate and know...</td>\n      <td>renewable energy, energy efficiency, sustainab...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1088</td>\n      <td>Manager, Clean Energy Asset Management</td>\n      <td>Meta</td>\n      <td>United States</td>\n      <td>https://www.linkedin.com/jobs/view/manager-cle...</td>\n      <td>The Manager of Clean Energy Asset Management w...</td>\n      <td>renewable energy, solar, wind, sustainability,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}